{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d9e91bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries (offline setup)\n",
    "%pip install transformers torch pandas numpy scikit-learn nltk rouge-score joblib -q\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "try:\n",
    "    from rouge import Rouge\n",
    "except ImportError:\n",
    "    Rouge = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8436ae0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking combined file: C:\\Users\\Admin\\Desktop\\Dataset\\conversationfile.xlsx\n",
      "read_excel failed, trying read_csv fallback -> Excel file format cannot be determined, you must specify an engine manually.\n",
      "Loaded as CSV fallback with 22 rows; columns: ['Conversation ID', 'Timestamp', 'Sender', 'Message']\n",
      "Preview (first rows):\n",
      "   Conversation ID            Timestamp  Sender  \\\n",
      "0                1  2025-10-07 10:15:12  User B   \n",
      "1                1  2025-10-07 10:15:45  User A   \n",
      "2                1  2025-10-07 10:16:05  User B   \n",
      "3                1  2025-10-07 10:16:38  User A   \n",
      "4                1  2025-10-07 10:17:01  User B   \n",
      "\n",
      "                                                                       Message  \n",
      "0                       Hey, did you see the client's feedback on the mockups?  \n",
      "1                 Just saw it. They want a lot of changes to the color scheme.  \n",
      "2  Yeah, that's what I was thinking. It's a big shift from the original brief.  \n",
      "3            I'll start on the revisions. Can you update the project timeline?  \n",
      "4                         Will do. I'll block out the rest of the week for it.  \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Configuration and quick validation\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Update these paths if your files are located elsewhere\n",
    "PATH_A = r\"C:\\Users\\Admin\\Desktop\\Dataset\\userA_chats.csv\"\n",
    "PATH_B = r\"C:\\Users\\Admin\\Desktop\\Dataset\\userB_chats.csv\"\n",
    "\n",
    "# If you prefer a single combined file (Excel/CSV), set USE_COMBINED=True and provide COMBINED_PATH\n",
    "USE_COMBINED = True\n",
    "COMBINED_PATH = r\"C:\\Users\\Admin\\Desktop\\Dataset\\conversationfile.xlsx\"\n",
    "\n",
    "# Controls\n",
    "RUN_TRAINING = False  # Set to True to run training\n",
    "DEV_RUN = True        # Set to True to use a tiny subset for fast tests\n",
    "JOBLIB_OUT = 'Model.joblib'\n",
    "\n",
    "\n",
    "def validate_paths(paths=(PATH_A, PATH_B), preview_lines=6):\n",
    "    for p in paths:\n",
    "        print(f\"Checking: {p}\")\n",
    "        if not os.path.exists(p):\n",
    "            print(\"MISSING\")\n",
    "            print(\" - Tip: place your CSV at this path or update PATH_A/PATH_B in the notebook.\")\n",
    "            print(\" - Expected columns: timestamp, message (optional: user_id, conversation_id)\")\n",
    "        else:\n",
    "            print(\"Exists -> preview:\")\n",
    "            try:\n",
    "                with open(p, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                    for i, line in enumerate(f):\n",
    "                        if i >= preview_lines:\n",
    "                            break\n",
    "                        print(line.rstrip())\n",
    "            except Exception as e:\n",
    "                print('Error reading file:', e)\n",
    "        print('----')\n",
    "\n",
    "\n",
    "def validate_combined(combined_path=COMBINED_PATH, preview_lines=6):\n",
    "    print(f\"Checking combined file: {combined_path}\")\n",
    "    if not os.path.exists(combined_path):\n",
    "        print('MISSING')\n",
    "        return\n",
    "    # Try to read with pandas: excel first, then csv fallback\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        try:\n",
    "            df = pd.read_excel(combined_path)\n",
    "            print(f'Loaded with pandas.read_excel: {len(df)} rows; columns: {list(df.columns[:10])}')\n",
    "            print('Preview (first rows):')\n",
    "            with pd.option_context('display.max_colwidth', 120):\n",
    "                print(df.head(5))\n",
    "        except Exception as e_excel:\n",
    "            print('read_excel failed, trying read_csv fallback ->', e_excel)\n",
    "            try:\n",
    "                df = pd.read_csv(combined_path, encoding='utf-8')\n",
    "                print(f'Loaded as CSV fallback with {len(df)} rows; columns: {list(df.columns[:10])}')\n",
    "                print('Preview (first rows):')\n",
    "                with pd.option_context('display.max_colwidth', 120):\n",
    "                    print(df.head(5))\n",
    "            except Exception as e_csv:\n",
    "                print('Could not read combined file as CSV either:', e_csv)\n",
    "    except Exception as e:\n",
    "        print('Pandas is not available or another error occurred:', e)\n",
    "    print('----')\n",
    "\n",
    "# Run quick validation when this cell is executed\n",
    "if USE_COMBINED:\n",
    "    validate_combined(COMBINED_PATH)\n",
    "else:\n",
    "    validate_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69aafe9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as CSV\n",
      "Detected columns:\n",
      " conversation_id -> Conversation ID\n",
      " timestamp       -> Timestamp\n",
      " sender          -> Sender\n",
      " message         -> Message\n",
      "Found 0 messages for User A and 0 messages for User B\n",
      "Wrote:\n",
      "  C:\\Users\\Admin\\Desktop\\Dataset\\userA_chats.csv\n",
      "  C:\\Users\\Admin\\Desktop\\Dataset\\userB_chats.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\Admin\\\\Desktop\\\\Dataset\\\\userA_chats.csv',\n",
       " 'C:\\\\Users\\\\Admin\\\\Desktop\\\\Dataset\\\\userB_chats.csv')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COMBINED_PATH = r\"C:\\Users\\Admin\\Downloads\\conversationfile.xlsx - userAuserB.csv\"\n",
    "\n",
    "import os, re\n",
    "import pandas as pd\n",
    "\n",
    "def split_combined_conversations(combined_path=COMBINED_PATH, path_a=PATH_A, path_b=PATH_B):\n",
    "    if not os.path.exists(combined_path):\n",
    "        print(f\"Combined file not found: {combined_path}\")\n",
    "        print(\"If you have a single combined CSV/Excel, set COMBINED_PATH accordingly or upload the file to the Dataset folder.\")\n",
    "        return None\n",
    "\n",
    "    # Try Excel first, then CSV\n",
    "    try:\n",
    "        df = pd.read_excel(combined_path)\n",
    "        print(\"Loaded as Excel\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            df = pd.read_csv(combined_path, encoding=\"utf-8\")\n",
    "            print(\"Loaded as CSV\")\n",
    "        except Exception as e:\n",
    "            print(\"Failed to read combined file:\", e)\n",
    "            return None\n",
    "\n",
    "    # helper to find columns by substring\n",
    "    def find_col(df, *keys):\n",
    "        keys = [k.lower() for k in keys]\n",
    "        return next((c for c in df.columns if any(k in c.lower() for k in keys)), None)\n",
    "\n",
    "    col_conv = find_col(df, \"conversation\")\n",
    "    col_time = find_col(df, \"time\", \"timestamp\", \"date\")\n",
    "    col_sender = find_col(df, \"sender\", \"user\", \"from\")\n",
    "    col_msg = find_col(df, \"message\", \"msg\", \"text\", \"content\")\n",
    "\n",
    "    print(\"Detected columns:\")\n",
    "    print(\" conversation_id ->\", col_conv)\n",
    "    print(\" timestamp       ->\", col_time)\n",
    "    print(\" sender          ->\", col_sender)\n",
    "    print(\" message         ->\", col_msg)\n",
    "\n",
    "    if col_msg is None or col_sender is None:\n",
    "        print('Required columns not found. Ensure the file has at least \"sender\" and \"message\" columns.')\n",
    "        return None\n",
    "\n",
    "    # rename detected columns to canonical names\n",
    "    rename_map = {}\n",
    "    if col_msg and col_msg != \"message\":     rename_map[col_msg] = \"message\"\n",
    "    if col_time and col_time != \"timestamp\": rename_map[col_time] = \"timestamp\"\n",
    "    if col_conv and col_conv != \"conversation_id\": rename_map[col_conv] = \"conversation_id\"\n",
    "    if col_sender and col_sender != \"sender\": rename_map[col_sender] = \"sender\"\n",
    "    if rename_map:\n",
    "        df = df.rename(columns=rename_map)\n",
    "\n",
    "    # clean text\n",
    "    df[\"message\"] = df[\"message\"].astype(str).str.replace(\"\\n\", \" \").str.strip()\n",
    "\n",
    "    # normalize sender to 'A' or 'B'\n",
    "    def map_sender(x):\n",
    "        if pd.isna(x):\n",
    "            return None\n",
    "        s = re.sub(r\"[\\s_\\-]+\", \"\", str(x).strip().lower())\n",
    "        if not s:\n",
    "            return None\n",
    "        if s[0] == \"a\":\n",
    "            return \"A\"\n",
    "        if s[0] == \"b\":\n",
    "            return \"B\"\n",
    "        return str(x)\n",
    "\n",
    "    df[\"sender_norm\"] = df[\"sender\"].apply(map_sender)\n",
    "\n",
    "    if \"timestamp\" in df.columns:\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "\n",
    "    dfA = df[df[\"sender_norm\"] == \"A\"].copy()\n",
    "    dfB = df[df[\"sender_norm\"] == \"B\"].copy()\n",
    "    print(f\"Found {len(dfA)} messages for User A and {len(dfB)} messages for User B\")\n",
    "\n",
    "    # ensure directories exist\n",
    "    for target in (path_a, path_b):\n",
    "        pdir = os.path.dirname(target)\n",
    "        if pdir and not os.path.exists(pdir):\n",
    "            os.makedirs(pdir, exist_ok=True)\n",
    "\n",
    "    save_cols = [c for c in (\"conversation_id\", \"timestamp\", \"message\") if c in df.columns] or [\"message\"]\n",
    "    dfA.to_csv(path_a, index=False, columns=save_cols)\n",
    "    dfB.to_csv(path_b, index=False, columns=save_cols)\n",
    "\n",
    "    print(\"Wrote:\")\n",
    "    print(\" \", path_a)\n",
    "    print(\" \", path_b)\n",
    "    return (path_a, path_b)\n",
    "\n",
    "# Run splitting helper (will print results)\n",
    "split_combined_conversations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5b52e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Total pairs: 10\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing: load and prepare conversations (accepts combined file directly)\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_prepare_data(pathA=None, pathB=None, combined_path=None, min_context=3):\n",
    "    \n",
    "   \n",
    "    def _from_combined(df):\n",
    "        # normalize column names\n",
    "        lower = {c.lower(): c for c in df.columns}\n",
    "        def find_col(key_substrs):\n",
    "            for k in lower:\n",
    "                lk = k.lower()\n",
    "                for s in key_substrs:\n",
    "                    if s in lk:\n",
    "                        return lower[k]\n",
    "            return None\n",
    "        col_time = find_col(['time', 'timestamp', 'date'])\n",
    "        col_sender = find_col(['sender', 'user', 'from'])\n",
    "        col_msg = find_col(['message', 'msg', 'text', 'content'])\n",
    "        if col_msg is None or col_sender is None:\n",
    "            raise ValueError('Combined file must contain at least sender and message columns')\n",
    "        # rename\n",
    "        if col_msg != 'message':\n",
    "            df = df.rename(columns={col_msg: 'message'})\n",
    "        if col_time and col_time != 'timestamp':\n",
    "            df = df.rename(columns={col_time: 'timestamp'})\n",
    "        if col_sender and col_sender != 'sender':\n",
    "            df = df.rename(columns={col_sender: 'sender'})\n",
    "        # clean message\n",
    "        df['message'] = df['message'].astype(str).str.replace('\\n', ' ').str.strip()\n",
    "        # normalize sender to 'A'/'B'\n",
    "        def map_sender(x):\n",
    "            if pd.isna(x):\n",
    "                return None\n",
    "            s = str(x).strip().lower()\n",
    "            if s in ('user a', 'a', 'user_a', 'user-a') or s == 'a':\n",
    "                return 'A'\n",
    "            if s in ('user b', 'b', 'user_b', 'user-b') or s == 'b':\n",
    "                return 'B'\n",
    "            # fallback: try to extract last character if it's A/B\n",
    "            s_clean = s.replace(' ', '').replace('_','').replace('-','')\n",
    "            if s_clean.endswith('a'):\n",
    "                return 'A'\n",
    "            if s_clean.endswith('b'):\n",
    "                return 'B'\n",
    "            return s\n",
    "        df['speaker'] = df['sender'].apply(map_sender)\n",
    "        # convert timestamp\n",
    "        if 'timestamp' in df.columns:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "        else:\n",
    "            # if no timestamp, create a sequential index to preserve order\n",
    "            df['timestamp'] = pd.RangeIndex(start=0, stop=len(df))\n",
    "        df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "        dfA = df[df['speaker'] == 'A'].copy()\n",
    "        dfB = df[df['speaker'] == 'B'].copy()\n",
    "        return dfA, dfB, df\n",
    "\n",
    "    # Load from combined\n",
    "    if combined_path is not None:\n",
    "        if not os.path.exists(combined_path):\n",
    "            raise FileNotFoundError(f'Combined file not found: {combined_path}')\n",
    "        try:\n",
    "            df_comb = pd.read_excel(combined_path)\n",
    "        except Exception:\n",
    "            df_comb = pd.read_csv(combined_path, encoding='utf-8')\n",
    "        dfA, dfB, merged = _from_combined(df_comb)\n",
    "    else:\n",
    "        # require both paths\n",
    "        if pathA is None or pathB is None:\n",
    "            raise ValueError('Provide either combined_path or both pathA and pathB')\n",
    "        dfA = pd.read_csv(pathA)\n",
    "        dfB = pd.read_csv(pathB)\n",
    "        for df in (dfA, dfB):\n",
    "            if 'timestamp' not in df.columns:\n",
    "                df['timestamp'] = pd.RangeIndex(start=0, stop=len(df))\n",
    "            if 'message' not in df.columns:\n",
    "                df['message'] = ''\n",
    "            df['message'] = df['message'].astype(str).str.replace('\\n', ' ').str.strip()\n",
    "        dfA['speaker'] = 'A'\n",
    "        dfB['speaker'] = 'B'\n",
    "        merged = pd.concat([dfA, dfB], ignore_index=True).sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "    # Build conversation sliding windows where the target response is by 'A'\n",
    "    conversations = []\n",
    "    for i in range(min_context, len(merged)):\n",
    "        if merged.loc[i, 'speaker'] == 'A':\n",
    "            context_parts = []\n",
    "            for j in range(i-min_context, i):\n",
    "                s = merged.loc[j, 'speaker']\n",
    "                m = merged.loc[j, 'message']\n",
    "                context_parts.append(f\"{s}: {m}\")\n",
    "            context = ' | '.join(context_parts)\n",
    "            response = merged.loc[i, 'message']\n",
    "            conversations.append(f\"{context} <SEP> A: {response}\")\n",
    "\n",
    "    return pd.DataFrame({'text': conversations})\n",
    "\n",
    "# Load data (select combined or separated files based on config flags)\n",
    "print('Loading data...')\n",
    "try:\n",
    "    if 'USE_COMBINED' in globals() and USE_COMBINED:\n",
    "        data = load_and_prepare_data(combined_path=COMBINED_PATH)\n",
    "    else:\n",
    "        data = load_and_prepare_data(pathA=PATH_A, pathB=PATH_B)\n",
    "    print(f'Total pairs: {len(data)}')\n",
    "except Exception as e:\n",
    "    print('Error while preparing data:', e)\n",
    "    data = pd.DataFrame({'text': []})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5af09fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer & model...\n",
      "Train: 8 | Val: 2\n",
      "Train: 8 | Val: 2\n"
     ]
    }
   ],
   "source": [
    "# Tokenization, Dataset and Model Init (short fallback)\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len=256):\n",
    "        self.texts = texts.tolist() if hasattr(texts, 'tolist') else list(texts)\n",
    "        if self.texts:\n",
    "            self.encodings = tokenizer(self.texts, truncation=True, padding='max_length', max_length=max_len, return_tensors='pt')\n",
    "        else:\n",
    "            import torch as _t\n",
    "            self.encodings = {'input_ids': _t.empty((0, max_len), dtype=_t.long), 'attention_mask': _t.empty((0, max_len), dtype=_t.long)}\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, i):\n",
    "        return {'input_ids': self.encodings['input_ids'][i], 'attention_mask': self.encodings['attention_mask'][i], 'labels': self.encodings['input_ids'][i]}\n",
    "\n",
    "print('Loading tokenizer & model...')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "\n",
    "if 'data' not in globals() or data is None:\n",
    "    data = None\n",
    "    if 'load_and_prepare_data' in globals():\n",
    "        try:\n",
    "            data = load_and_prepare_data(combined_path=COMBINED_PATH) if globals().get('USE_COMBINED') else load_and_prepare_data(pathA=PATH_A, pathB=PATH_B)\n",
    "        except Exception as e:\n",
    "            print('helper failed ->', e)\n",
    "            data = None\n",
    "    if data is None:\n",
    "        try:\n",
    "            if globals().get('USE_COMBINED') and os.path.exists(COMBINED_PATH):\n",
    "                try:\n",
    "                    df = pd.read_excel(COMBINED_PATH)\n",
    "                except Exception:\n",
    "                    df = pd.read_csv(COMBINED_PATH, encoding='utf-8')\n",
    "                msg = next((c for c in df.columns if 'message' in c.lower() or 'text' in c.lower()), None)\n",
    "                snd = next((c for c in df.columns if 'sender' in c.lower() or 'user' in c.lower()), None)\n",
    "                ts  = next((c for c in df.columns if 'time' in c.lower() or 'date' in c.lower()), None)\n",
    "                if msg and snd:\n",
    "                    df['message'] = df[msg].astype(str).str.replace('\\n',' ').str.strip()\n",
    "                    df['speaker'] = df[snd].astype(str).str.strip().str.lower().str[0].map({'a':'A','b':'B'})\n",
    "                    if ts is not None:\n",
    "                        try:\n",
    "                            df[ts] = pd.to_datetime(df[ts], errors='coerce')\n",
    "                            df = df.sort_values(ts).reset_index(drop=True)\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                    merged = df\n",
    "                    convs = []\n",
    "                    for i in range(3, len(merged)):\n",
    "                        if merged.loc[i, 'speaker'] == 'A':\n",
    "                            ctx = ' | '.join(f\"{merged.loc[j,'speaker']}: {merged.loc[j,'message']}\" for j in range(i-3, i))\n",
    "                            convs.append(f\"{ctx} <SEP> A: {merged.loc[i,'message']}\")\n",
    "                    data = pd.DataFrame({'text': convs})\n",
    "                else:\n",
    "                    data = pd.DataFrame({'text': []})\n",
    "            else:\n",
    "                if os.path.exists(PATH_A) and os.path.exists(PATH_B):\n",
    "                    dfA = pd.read_csv(PATH_A); dfB = pd.read_csv(PATH_B)\n",
    "                    for d in (dfA, dfB):\n",
    "                        if 'message' not in d.columns:\n",
    "                            d['message'] = ''\n",
    "                    dfA['speaker'] = 'A'; dfB['speaker'] = 'B'\n",
    "                    merged = pd.concat([dfA, dfB], ignore_index=True).reset_index(drop=True)\n",
    "                    convs = []\n",
    "                    for i in range(3, len(merged)):\n",
    "                        if merged.loc[i, 'speaker'] == 'A':\n",
    "                            ctx = ' | '.join(f\"{merged.loc[j,'speaker']}: {merged.loc[j,'message']}\" for j in range(i-3, i))\n",
    "                            convs.append(f\"{ctx} <SEP> A: {merged.loc[i,'message']}\")\n",
    "                    data = pd.DataFrame({'text': convs})\n",
    "                else:\n",
    "                    data = pd.DataFrame({'text': []})\n",
    "        except Exception as e:\n",
    "            print('Fallback load failed ->', e)\n",
    "            data = pd.DataFrame({'text': []})\n",
    "\n",
    "texts = pd.Series(data['text']) if isinstance(data, pd.DataFrame) and 'text' in data.columns else pd.Series(list(data) if data is not None else [], dtype=object)\n",
    "texts = texts.reset_index(drop=True)\n",
    "# Split safely\n",
    "if len(texts) < 2:\n",
    "    train_texts, val_texts = texts, texts.iloc[:0]\n",
    "else:\n",
    "    train_texts, val_texts = map(pd.Series, train_test_split(texts, test_size=0.15, random_state=42))\n",
    "if globals().get('DEV_RUN'):\n",
    "    train_texts, val_texts = train_texts.iloc[:32], val_texts.iloc[:8]\n",
    "train_dataset, val_dataset = ChatDataset(train_texts, tokenizer), ChatDataset(val_texts, tokenizer)\n",
    "print(f'Train: {len(train_dataset)} | Val: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0404579a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV_RUN=True and RUN_TRAINING=False — enabling a quick smoke run for this session\n",
      "🚀 Starting training (smoke-run settings if DEV_RUN=True)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.373800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.865200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.024600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.757800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.615800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.247100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.189600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.207900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training complete!\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "\n",
    "_dev = globals().get('DEV_RUN', False)\n",
    "RUN_TRAINING = globals().get('RUN_TRAINING', False)\n",
    "\n",
    "if _dev and not RUN_TRAINING:\n",
    "    print('DEV_RUN=True and RUN_TRAINING=False — enabling a quick smoke run for this session')\n",
    "    _epochs = 1\n",
    "    _batch = 1\n",
    "    _logging = 1\n",
    "    RUN_TRAINING = True\n",
    "else:\n",
    "    _epochs = 3\n",
    "    _batch = 4\n",
    "    _logging = 50\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./chat_model',\n",
    "    num_train_epochs=_epochs,\n",
    "    per_device_train_batch_size=_batch,\n",
    "    per_device_eval_batch_size=_batch,\n",
    "    warmup_steps=10 if _dev else 100,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=_logging,\n",
    "    do_eval=True,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    learning_rate=5e-5\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "if RUN_TRAINING:\n",
    "    print(\"🚀 Starting training (smoke-run settings if DEV_RUN=True)...\")\n",
    "    trainer.train()\n",
    "    print(\"✅ Training complete!\")\n",
    "else:\n",
    "    print(\"RUN_TRAINING is False — skipping trainer.train()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1f7991d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Well, it's a good idea, so you\", \"Okay, we're\", 'Well, I\\'m really confused. I have a little bit of a story.\\n\\n<SE: Yeah, I can tell you, \"I\\'m a little bit of a kid. And I am a kid. I have a good little story, and I\\'m happy to talk to you.\"\\n\\n<P> I guess that\\'s the point. I think you could']\n"
     ]
    }
   ],
   "source": [
    "# Generation utility\n",
    "def generate_reply(context, model, tokenizer, num_replies=3, max_length=100):\n",
    "    input_text = f\"{context} <SEP> A:\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    if torch.cuda.is_available():\n",
    "        input_ids = input_ids.to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_replies,\n",
    "        temperature=0.8,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    replies = [tokenizer.decode(out, skip_special_tokens=True).split('A:')[-1].strip() for out in outputs]\n",
    "    return replies\n",
    "\n",
    "\n",
    "context = \"B: How are you? | A: Good! | B: What's up?\"\n",
    "print(generate_reply(context, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aa114df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation utilities\n",
    "def calculate_metrics(references, predictions, model=None, tokenizer=None):\n",
    "    smooth = SmoothingFunction()\n",
    "    bleu_scores = []\n",
    "    for ref, pred in zip(references, predictions):\n",
    "        try:\n",
    "            s = sentence_bleu([ref.split()], pred.split(), smoothing_function=smooth.method1)\n",
    "        except Exception:\n",
    "            s = 0.0\n",
    "        bleu_scores.append(s)\n",
    "    bleu = float(np.mean(bleu_scores)) if bleu_scores else 0.0\n",
    "\n",
    "    if Rouge is not None:\n",
    "        try:\n",
    "            rouge_scores = Rouge().get_scores(predictions, references, avg=True)\n",
    "        except Exception:\n",
    "            rouge_scores = {'rouge-1': {'f': 0.0}, 'rouge-2': {'f': 0.0}, 'rouge-l': {'f': 0.0}}\n",
    "    else:\n",
    "        rouge_scores = {'rouge-1': {'f': 0.0}, 'rouge-2': {'f': 0.0}, 'rouge-l': {'f': 0.0}}\n",
    "\n",
    "    if model is not None and tokenizer is not None and len(references) > 0:\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        n = min(50, len(references))\n",
    "        with torch.no_grad():\n",
    "            for text in references[:n]:\n",
    "                inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=256)\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "                outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "                total_loss += outputs.loss.item()\n",
    "        perplexity = float(np.exp(total_loss / n)) if n > 0 else float('inf')\n",
    "    else:\n",
    "        perplexity = float('inf')\n",
    "\n",
    "    return bleu, rouge_scores, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7cf15fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_pipeline_package(save_path=JOBLIB_OUT, bleu=0.0, rouge=None, perplexity=float('inf')):\n",
    "    rouge = rouge or {'rouge-1': {'f': 0.0}, 'rouge-2': {'f': 0.0}, 'rouge-l': {'f': 0.0}}\n",
    "    pipeline_package = {\n",
    "        'model_name': 'gpt2-chat-finetuned',\n",
    "        'tokenizer': tokenizer,\n",
    "        'generation_config': {\n",
    "            'max_length': 100,\n",
    "            'temperature': 0.8,\n",
    "            'top_k': 50,\n",
    "            'top_p': 0.95\n",
    "        },\n",
    "        'metrics': {\n",
    "            'bleu': float(bleu),\n",
    "            'rouge_1': float(rouge['rouge-1']['f']),\n",
    "            'rouge_2': float(rouge['rouge-2']['f']),\n",
    "            'rouge_l': float(rouge['rouge-l']['f']),\n",
    "            'perplexity': float(perplexity)\n",
    "        }\n",
    "    }\n",
    "    joblib.dump(pipeline_package, save_path)\n",
    "    print(f'\\nModel package saved as \"{save_path}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7134f8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 8\n",
      "Val dataset length:   2\n",
      "Example 1: A: \"How about around 3 PM?\" | A: \"Are you free? My laptop just went blank.\" | B: \"Oh no. Did you try a hard reboot?\" <SEP> A: \"Tried it twice. Nothing.\"\n",
      "Example 2: B: \"Hey, did you see the client's feedback on the mockups?\" | A: \"Just saw it. They want a lot of changes to the color scheme.\" | B: \"Yeah, that's what I was thinking. It's a big shift from the original brief.\" <SEP> A: \"I'll start on the revisions. Can you update the project timeline?\"\n",
      "Example 3: B: \"Okay, try connecting it to an external monitor. Maybe the display is the issue.\" | A: \"Good idea, let me find a cable.\" | B: \"Let me know if that works. If not, we might have to call IT.\" <SEP> A: \"Finally watched that new sci-fi movie everyone's talking about.\"\n",
      "\n",
      "To run training now: set RUN_TRAINING=True and (optionally) DEV_RUN=True for a quick smoke run, then re-run the Training cell.\n"
     ]
    }
   ],
   "source": [
    "# Quick preview: dataset sizes and up to 3 training examples\n",
    "print('Train dataset length:', len(train_dataset))\n",
    "print('Val dataset length:  ', len(val_dataset))\n",
    "\n",
    "try:\n",
    "    for i in range(min(3, len(train_dataset))):\n",
    "        ids = train_dataset[i]['input_ids']\n",
    "        txt = tokenizer.decode(ids, skip_special_tokens=True).strip()\n",
    "        print(f\"Example {i+1}: {txt}\")\n",
    "except Exception as e:\n",
    "    print('Could not decode preview examples:', e)\n",
    "\n",
    "print('\\nTo run training now: set RUN_TRAINING=True and (optionally) DEV_RUN=True for a quick smoke run, then re-run the Training cell.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
