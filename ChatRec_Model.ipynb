{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0418c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script isympy.exe is installed in 'c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts hf.exe, huggingface-cli.exe and tiny-agents.exe are installed in 'c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts transformers-cli.exe and transformers.exe are installed in 'c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries (offline setup)\n",
    "%pip install transformers torch pandas numpy scikit-learn nltk rouge-score joblib -q\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "try:\n",
    "    from rouge import Rouge\n",
    "except ImportError:\n",
    "    Rouge = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130aa1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and quick validation\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Update these paths if your files are located elsewhere\n",
    "PATH_A = r\"C:\\Users\\Admin\\Desktop\\Dataset\\userA_chats.csv\"\n",
    "PATH_B = r\"C:\\Users\\Admin\\Desktop\\Dataset\\userB_chats.csv\"\n",
    "\n",
    "# If you prefer a single combined file (Excel/CSV), set USE_COMBINED=True and provide COMBINED_PATH\n",
    "USE_COMBINED = True\n",
    "COMBINED_PATH = r\"C:\\Users\\Admin\\Desktop\\Dataset\\conversationfile.xlsx\"\n",
    "\n",
    "# Controls\n",
    "RUN_TRAINING = False  # Set to True to run training\n",
    "DEV_RUN = True        # Set to True to use a tiny subset for fast tests\n",
    "JOBLIB_OUT = 'Model.joblib'\n",
    "\n",
    "\n",
    "def validate_paths(paths=(PATH_A, PATH_B), preview_lines=6):\n",
    "    \"\"\"Check existence of dataset files and print a small preview (header + few rows).\n",
    "    Prints clear guidance if files are missing or unreadable.\n",
    "    \"\"\"\n",
    "    for p in paths:\n",
    "        print(f\"Checking: {p}\")\n",
    "        if not os.path.exists(p):\n",
    "            print(\"MISSING\")\n",
    "            print(\" - Tip: place your CSV at this path or update PATH_A/PATH_B in the notebook.\")\n",
    "            print(\" - Expected columns: timestamp, message (optional: user_id, conversation_id)\")\n",
    "        else:\n",
    "            print(\"Exists -> preview:\")\n",
    "            try:\n",
    "                with open(p, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                    for i, line in enumerate(f):\n",
    "                        if i >= preview_lines:\n",
    "                            break\n",
    "                        print(line.rstrip())\n",
    "            except Exception as e:\n",
    "                print('Error reading file:', e)\n",
    "        print('----')\n",
    "\n",
    "\n",
    "def validate_combined(combined_path=COMBINED_PATH, preview_lines=6):\n",
    "    \"\"\"Validate combined Excel/CSV file existence and print a brief preview.\n",
    "    Tries to read as Excel first, but falls back to CSV when read_excel fails.\n",
    "    \"\"\"\n",
    "    print(f\"Checking combined file: {combined_path}\")\n",
    "    if not os.path.exists(combined_path):\n",
    "        print('MISSING')\n",
    "        print(' - Tip: place the combined conversation file at COMBINED_PATH or update the path above.')\n",
    "        return\n",
    "    # Try to read with pandas: excel first, then csv fallback\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        try:\n",
    "            df = pd.read_excel(combined_path)\n",
    "            print(f'Loaded with pandas.read_excel: {len(df)} rows; columns: {list(df.columns[:10])}')\n",
    "            print('Preview (first rows):')\n",
    "            with pd.option_context('display.max_colwidth', 120):\n",
    "                print(df.head(5))\n",
    "        except Exception as e_excel:\n",
    "            print('read_excel failed, trying read_csv fallback ->', e_excel)\n",
    "            try:\n",
    "                df = pd.read_csv(combined_path, encoding='utf-8')\n",
    "                print(f'Loaded as CSV fallback with {len(df)} rows; columns: {list(df.columns[:10])}')\n",
    "                print('Preview (first rows):')\n",
    "                with pd.option_context('display.max_colwidth', 120):\n",
    "                    print(df.head(5))\n",
    "            except Exception as e_csv:\n",
    "                print('Could not read combined file as CSV either:', e_csv)\n",
    "    except Exception as e:\n",
    "        print('Pandas is not available or another error occurred:', e)\n",
    "    print('----')\n",
    "\n",
    "# Run quick validation when this cell is executed\n",
    "if USE_COMBINED:\n",
    "    validate_combined(COMBINED_PATH)\n",
    "else:\n",
    "    validate_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9c5785",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "COMBINED_PATH = r\"C:\\Users\\Admin\\Downloads\\conversationfile.xlsx - userAuserB.csv\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def split_combined_conversations(combined_path=COMBINED_PATH, path_a=PATH_A, path_b=PATH_B):\n",
    "    \"\"\"Load a combined conversation file and write two CSVs: one for User A and one for User B.\n",
    "    Expects columns (case-insensitive): conversation_id, timestamp, sender, message\n",
    "    \"\"\"\n",
    "    if not os.path.exists(combined_path):\n",
    "        print(f\"Combined file not found: {combined_path}\")\n",
    "        print(\"If you have a single combined CSV/Excel, set COMBINED_PATH accordingly or upload the file to the Dataset folder.\")\n",
    "        return None\n",
    "\n",
    "    # Try Excel first, then CSV\n",
    "    try:\n",
    "        df = pd.read_excel(combined_path)\n",
    "        print('Loaded as Excel')\n",
    "    except Exception:\n",
    "        try:\n",
    "            df = pd.read_csv(combined_path, encoding='utf-8')\n",
    "            print('Loaded as CSV')\n",
    "        except Exception as e:\n",
    "            print('Failed to read combined file:', e)\n",
    "            return None\n",
    "\n",
    "    # Normalize column names to lower-case keys\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    lower = {k.lower(): k for k in df.columns}\n",
    "\n",
    "    # Detect required columns\n",
    "    def find_col(key_substrs):\n",
    "        for k in lower:\n",
    "            lk = k.lower()\n",
    "            for s in key_substrs:\n",
    "                if s in lk:\n",
    "                    return lower[k]\n",
    "        return None\n",
    "\n",
    "    col_conv = find_col(['conversation'])\n",
    "    col_time = find_col(['time', 'timestamp', 'date'])\n",
    "    col_sender = find_col(['sender', 'user', 'from'])\n",
    "    col_msg = find_col(['message', 'msg', 'text', 'content'])\n",
    "\n",
    "    print('Detected columns:')\n",
    "    print(' conversation_id ->', col_conv)\n",
    "    print(' timestamp       ->', col_time)\n",
    "    print(' sender          ->', col_sender)\n",
    "    print(' message         ->', col_msg)\n",
    "\n",
    "    if col_msg is None or col_sender is None:\n",
    "        print('Required columns not found. Ensure the file has at least \"sender\" and \"message\" columns.')\n",
    "        return None\n",
    "\n",
    "    # Keep needed columns and sanitize\n",
    "    df = df.rename(columns={col_msg: 'message'}) if col_msg != 'message' else df\n",
    "    if col_time:\n",
    "        df = df.rename(columns={col_time: 'timestamp'}) if col_time != 'timestamp' else df\n",
    "    if col_conv:\n",
    "        df = df.rename(columns={col_conv: 'conversation_id'}) if col_conv != 'conversation_id' else df\n",
    "    if col_sender:\n",
    "        df = df.rename(columns={col_sender: 'sender'}) if col_sender != 'sender' else df\n",
    "\n",
    "    # Clean message text\n",
    "    df['message'] = df['message'].astype(str).str.replace('\\n', ' ').str.strip()\n",
    "\n",
    "    # Normalize sender values to 'A' or 'B'\n",
    "    def map_sender(x):\n",
    "        if pd.isna(x):\n",
    "            return None\n",
    "        s = str(x).strip().lower()\n",
    "        if s in ('user a', 'a', 'user_a', 'user-a'):\n",
    "            return 'A'\n",
    "        if s in ('user b', 'b', 'user_b', 'user-b'):\n",
    "            return 'B'\n",
    "        # catch common patterns\n",
    "        if s.startswith('user a') or s == 'a':\n",
    "            return 'A'\n",
    "        if s.startswith('user b') or s == 'b':\n",
    "            return 'B'\n",
    "        # If sender contains just a single letter\n",
    "        if s == 'a':\n",
    "            return 'A'\n",
    "        if s == 'b':\n",
    "            return 'B'\n",
    "        return s  # fallback: keep original\n",
    "\n",
    "    df['sender_norm'] = df['sender'].apply(map_sender)\n",
    "\n",
    "    # Convert timestamp where present\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "\n",
    "    # Split\n",
    "    dfA = df[df['sender_norm'] == 'A'].copy()\n",
    "    dfB = df[df['sender_norm'] == 'B'].copy()\n",
    "\n",
    "    print(f'Found {len(dfA)} messages for User A and {len(dfB)} messages for User B')\n",
    "\n",
    "    # Ensure target directory exists\n",
    "    for target in (path_a, path_b):\n",
    "        pdir = os.path.dirname(target)\n",
    "        if pdir and not os.path.exists(pdir):\n",
    "            os.makedirs(pdir, exist_ok=True)\n",
    "\n",
    "    # Save with consistent columns (timestamp, message, conversation_id)\n",
    "    save_cols = [c for c in ['conversation_id', 'timestamp', 'message'] if c in df.columns]\n",
    "    if save_cols == []:\n",
    "        save_cols = ['message']\n",
    "\n",
    "    dfA.to_csv(path_a, index=False, columns=save_cols)\n",
    "    dfB.to_csv(path_b, index=False, columns=save_cols)\n",
    "\n",
    "    print('Wrote:')\n",
    "    print(' ', path_a)\n",
    "    print(' ', path_b)\n",
    "    return (path_a, path_b)\n",
    "\n",
    "# Run splitting helper (will print results)\n",
    "split_combined_conversations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebdff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: load and prepare conversations (accepts combined file directly)\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_prepare_data(pathA=None, pathB=None, combined_path=None, min_context=3):\n",
    "    \n",
    "   \n",
    "    def _from_combined(df):\n",
    "        # normalize column names\n",
    "        lower = {c.lower(): c for c in df.columns}\n",
    "        def find_col(key_substrs):\n",
    "            for k in lower:\n",
    "                lk = k.lower()\n",
    "                for s in key_substrs:\n",
    "                    if s in lk:\n",
    "                        return lower[k]\n",
    "            return None\n",
    "        col_time = find_col(['time', 'timestamp', 'date'])\n",
    "        col_sender = find_col(['sender', 'user', 'from'])\n",
    "        col_msg = find_col(['message', 'msg', 'text', 'content'])\n",
    "        if col_msg is None or col_sender is None:\n",
    "            raise ValueError('Combined file must contain at least sender and message columns')\n",
    "        # rename\n",
    "        if col_msg != 'message':\n",
    "            df = df.rename(columns={col_msg: 'message'})\n",
    "        if col_time and col_time != 'timestamp':\n",
    "            df = df.rename(columns={col_time: 'timestamp'})\n",
    "        if col_sender and col_sender != 'sender':\n",
    "            df = df.rename(columns={col_sender: 'sender'})\n",
    "        # clean message\n",
    "        df['message'] = df['message'].astype(str).str.replace('\\n', ' ').str.strip()\n",
    "        # normalize sender to 'A'/'B'\n",
    "        def map_sender(x):\n",
    "            if pd.isna(x):\n",
    "                return None\n",
    "            s = str(x).strip().lower()\n",
    "            if s in ('user a', 'a', 'user_a', 'user-a') or s == 'a':\n",
    "                return 'A'\n",
    "            if s in ('user b', 'b', 'user_b', 'user-b') or s == 'b':\n",
    "                return 'B'\n",
    "            # fallback: try to extract last character if it's A/B\n",
    "            s_clean = s.replace(' ', '').replace('_','').replace('-','')\n",
    "            if s_clean.endswith('a'):\n",
    "                return 'A'\n",
    "            if s_clean.endswith('b'):\n",
    "                return 'B'\n",
    "            return s\n",
    "        df['speaker'] = df['sender'].apply(map_sender)\n",
    "        # convert timestamp\n",
    "        if 'timestamp' in df.columns:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "        else:\n",
    "            # if no timestamp, create a sequential index to preserve order\n",
    "            df['timestamp'] = pd.RangeIndex(start=0, stop=len(df))\n",
    "        df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "        dfA = df[df['speaker'] == 'A'].copy()\n",
    "        dfB = df[df['speaker'] == 'B'].copy()\n",
    "        return dfA, dfB, df\n",
    "\n",
    "    # Load from combined\n",
    "    if combined_path is not None:\n",
    "        if not os.path.exists(combined_path):\n",
    "            raise FileNotFoundError(f'Combined file not found: {combined_path}')\n",
    "        try:\n",
    "            df_comb = pd.read_excel(combined_path)\n",
    "        except Exception:\n",
    "            df_comb = pd.read_csv(combined_path, encoding='utf-8')\n",
    "        dfA, dfB, merged = _from_combined(df_comb)\n",
    "    else:\n",
    "        # require both paths\n",
    "        if pathA is None or pathB is None:\n",
    "            raise ValueError('Provide either combined_path or both pathA and pathB')\n",
    "        dfA = pd.read_csv(pathA)\n",
    "        dfB = pd.read_csv(pathB)\n",
    "        for df in (dfA, dfB):\n",
    "            if 'timestamp' not in df.columns:\n",
    "                df['timestamp'] = pd.RangeIndex(start=0, stop=len(df))\n",
    "            if 'message' not in df.columns:\n",
    "                df['message'] = ''\n",
    "            df['message'] = df['message'].astype(str).str.replace('\\n', ' ').str.strip()\n",
    "        dfA['speaker'] = 'A'\n",
    "        dfB['speaker'] = 'B'\n",
    "        merged = pd.concat([dfA, dfB], ignore_index=True).sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "    # Build conversation sliding windows where the target response is by 'A'\n",
    "    conversations = []\n",
    "    for i in range(min_context, len(merged)):\n",
    "        if merged.loc[i, 'speaker'] == 'A':\n",
    "            context_parts = []\n",
    "            for j in range(i-min_context, i):\n",
    "                s = merged.loc[j, 'speaker']\n",
    "                m = merged.loc[j, 'message']\n",
    "                context_parts.append(f\"{s}: {m}\")\n",
    "            context = ' | '.join(context_parts)\n",
    "            response = merged.loc[i, 'message']\n",
    "            conversations.append(f\"{context} <SEP> A: {response}\")\n",
    "\n",
    "    return pd.DataFrame({'text': conversations})\n",
    "\n",
    "# Load data (select combined or separated files based on config flags)\n",
    "print('Loading data...')\n",
    "try:\n",
    "    if 'USE_COMBINED' in globals() and USE_COMBINED:\n",
    "        data = load_and_prepare_data(combined_path=COMBINED_PATH)\n",
    "    else:\n",
    "        data = load_and_prepare_data(pathA=PATH_A, pathB=PATH_B)\n",
    "    print(f'Total pairs: {len(data)}')\n",
    "except Exception as e:\n",
    "    print('Error while preparing data:', e)\n",
    "    data = pd.DataFrame({'text': []})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dc7511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization, Dataset and Model Init\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len=256):\n",
    "        self.texts = texts.tolist() if hasattr(texts, 'tolist') else list(texts)\n",
    "        self.encodings = tokenizer(self.texts, truncation=True, padding='max_length', max_length=max_len, return_tensors='pt')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'labels': self.encodings['input_ids'][idx]\n",
    "        }\n",
    "\n",
    "print('Loading tokenizer & model...')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Create datasets (use small dev sample if DEV_RUN True)\n",
    "train_texts, val_texts = train_test_split(data['text'], test_size=0.15, random_state=42)\n",
    "if DEV_RUN:\n",
    "    train_texts = train_texts.iloc[:32]\n",
    "    val_texts = val_texts.iloc[:8]\n",
    "\n",
    "train_dataset = ChatDataset(train_texts, tokenizer)\n",
    "val_dataset = ChatDataset(val_texts, tokenizer)\n",
    "print(f'Train: {len(train_dataset)} | Val: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f70f1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup (guarded)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./chat_model',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=200,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    learning_rate=5e-5\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "if RUN_TRAINING:\n",
    "    print('Starting training...')\n",
    "    trainer.train()\n",
    "    print('✓ Training complete!')\n",
    "else:\n",
    "    print('RUN_TRAINING is False — skipping trainer.train()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a9acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation utility\n",
    "def generate_reply(context, model, tokenizer, num_replies=3, max_length=100):\n",
    "    input_text = f\"{context} <SEP> A:\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    if torch.cuda.is_available():\n",
    "        input_ids = input_ids.to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_replies,\n",
    "        temperature=0.8,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    replies = [tokenizer.decode(out, skip_special_tokens=True).split('A:')[-1].strip() for out in outputs]\n",
    "    return replies\n",
    "\n",
    "\n",
    "# context = \"B: How are you? | A: Good! | B: What's up?\"\n",
    "# print(generate_reply(context, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58bd1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation utilities\n",
    "def calculate_metrics(references, predictions, model=None, tokenizer=None):\n",
    "    smooth = SmoothingFunction()\n",
    "    bleu_scores = []\n",
    "    for ref, pred in zip(references, predictions):\n",
    "        try:\n",
    "            s = sentence_bleu([ref.split()], pred.split(), smoothing_function=smooth.method1)\n",
    "        except Exception:\n",
    "            s = 0.0\n",
    "        bleu_scores.append(s)\n",
    "    bleu = float(np.mean(bleu_scores)) if bleu_scores else 0.0\n",
    "\n",
    "    if Rouge is not None:\n",
    "        try:\n",
    "            rouge_scores = Rouge().get_scores(predictions, references, avg=True)\n",
    "        except Exception:\n",
    "            rouge_scores = {'rouge-1': {'f': 0.0}, 'rouge-2': {'f': 0.0}, 'rouge-l': {'f': 0.0}}\n",
    "    else:\n",
    "        rouge_scores = {'rouge-1': {'f': 0.0}, 'rouge-2': {'f': 0.0}, 'rouge-l': {'f': 0.0}}\n",
    "\n",
    "    if model is not None and tokenizer is not None and len(references) > 0:\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        n = min(50, len(references))\n",
    "        with torch.no_grad():\n",
    "            for text in references[:n]:\n",
    "                inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=256)\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "                outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "                total_loss += outputs.loss.item()\n",
    "        perplexity = float(np.exp(total_loss / n)) if n > 0 else float('inf')\n",
    "    else:\n",
    "        perplexity = float('inf')\n",
    "\n",
    "    return bleu, rouge_scores, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c41bd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'JOBLIB_OUT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave_pipeline_package\u001b[39m(save_path\u001b[38;5;241m=\u001b[39m\u001b[43mJOBLIB_OUT\u001b[49m, bleu\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, rouge\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, perplexity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m      2\u001b[0m     rouge \u001b[38;5;241m=\u001b[39m rouge \u001b[38;5;129;01mor\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge-1\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0\u001b[39m}, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge-2\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0\u001b[39m}, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge-l\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0\u001b[39m}}\n\u001b[0;32m      3\u001b[0m     pipeline_package \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2-chat-finetuned\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m: tokenizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m         }\n\u001b[0;32m     19\u001b[0m     }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'JOBLIB_OUT' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def save_pipeline_package(save_path=JOBLIB_OUT, bleu=0.0, rouge=None, perplexity=float('inf')):\n",
    "    rouge = rouge or {'rouge-1': {'f': 0.0}, 'rouge-2': {'f': 0.0}, 'rouge-l': {'f': 0.0}}\n",
    "    pipeline_package = {\n",
    "        'model_name': 'gpt2-chat-finetuned',\n",
    "        'tokenizer': tokenizer,\n",
    "        'generation_config': {\n",
    "            'max_length': 100,\n",
    "            'temperature': 0.8,\n",
    "            'top_k': 50,\n",
    "            'top_p': 0.95\n",
    "        },\n",
    "        'metrics': {\n",
    "            'bleu': float(bleu),\n",
    "            'rouge_1': float(rouge['rouge-1']['f']),\n",
    "            'rouge_2': float(rouge['rouge-2']['f']),\n",
    "            'rouge_l': float(rouge['rouge-l']['f']),\n",
    "            'perplexity': float(perplexity)\n",
    "        }\n",
    "    }\n",
    "    joblib.dump(pipeline_package, save_path)\n",
    "    print(f'\\nModel package saved as \"{save_path}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
